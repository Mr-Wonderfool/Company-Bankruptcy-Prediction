### BorderlineSMOTE
- 原始SMOTE方法容易模糊正负样本的边界，使得分类效果下降。`BorderlineSMOTE`将样本点分为噪声（如果超半数邻居属于另外的类）和边界（半数邻居属于另外的类）。

### 预处理
- 数据标准化
- 去除方差较小的列（数据重复度很高的列）
- 去除线性相关性较大（>=0.8）的列
- 最终保留57个特征进行互信息的筛选

### 特征选择
- 互信息
  - 没有数据分布的先验假设
  - 可以捕捉单个特征与目标之间的非线性关系
  - 只关注单个特征，没有考虑特征之间的高阶相关性
- 利用相关系数进行筛选
  - 只能捕捉特征与目标之间的线性关系
- F检验和$\chi^2$检验
  - 假设数据是正态分布
  - 利用假设检验的思想选择变量
**注意**：使用`f_classif`时观察到很小的p值，以及NaN值。数值上的不稳定性表明有些数据并不符合F检验对于组内数据服从正态分布的假设，所以选择使用互信息作为选择标准，并进一步检验组内数据的分布
- 利用互信息选择30%数量的特征：（展示变量重要性绘图）
```python
[0,2,3,6,8,9,22,24,26,34,36,38,41,45,53,57,58,60] # mutual information
[0,1,6,8,9,10,24,26,32,34,36,37,38,40,46,53,54,60] # f test
```

### 正态分布检验
- 首先数据**标准化**，从原数据表中提取相应的特征列，拆分成正负两类，利用`stats.probplot`做Q-Q图（排序后数据-正态分布理论分位数），由于数据已经标准化，Q-Q图的拟合曲线应当与$y=x$重合度较好，展示其中对于EPS（在互信息中得分最高）的正态检验。
- 逻辑链条：互信息选择的变量进行单个的高斯检验 -> 通过高斯检验 -> 所有变量放在一起服从多维高斯 -> 符合使用高斯判别分析的条件